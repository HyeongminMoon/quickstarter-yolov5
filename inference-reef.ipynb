{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421450a-46f3-4fbd-98ba-8d6bedd69169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import input.greatbarrierreef as gbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb86fb-9de0-4483-a8c3-0f616ffd062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gbr.make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a475657-6046-4db9-aeb3-0c97708bb54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ac6e3-911e-4c69-a12b-f81b09150658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for (img, df) in iter_test:\n",
    "    display(Image.fromarray(img))\n",
    "    df['annotations'] = '0.5 0 0 100 100'\n",
    "    env.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452b72e-43ef-43c7-af64-9a46f07c0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "FILE = Path(os.getcwd()).resolve()\n",
    "ROOT = FILE.parents[0]  # YOLOv5 root directory\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))  # add ROOT to PATH\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n",
    "\n",
    "from models.common import DetectMultiBackend\n",
    "from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
    "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
    "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
    "from utils.plots import Annotator, colors, save_one_box\n",
    "from utils.torch_utils import select_device, time_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433e95b-e15d-4e0f-bfe2-208e2088e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a9288-d8db-4dd2-b930-06ec6da7ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights='runs/train/yolov5x_fold08/weights/best.pt'  # model.pt path(s)\n",
    "source='convertor/fold0/images/val2017'  # file/dir/URL/glob, 0 for webcam\n",
    "imgsz=(720, 1280)  # inference size (height, width)\n",
    "conf_thres=0.25  # confidence threshold\n",
    "iou_thres=0.45  # NMS IOU threshold\n",
    "max_det=1000  # maximum detections per image\n",
    "device='cuda:0'  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "view_img=False  # show results\n",
    "save_txt=False  # save results to *.txt\n",
    "save_conf=False  # save confidences in --save-txt labels\n",
    "save_crop=False  # save cropped prediction boxes\n",
    "nosave=False  # do not save images/videos\n",
    "classes=None  # filter by class: --class 0, or --class 0 2 3\n",
    "agnostic_nms=False  # class-agnostic NMS\n",
    "augment=False  # augmented inference\n",
    "visualize=False  # visualize features\n",
    "update=False  # update all models\n",
    "project='runs/detect'  # save results to project/name\n",
    "name='exp'  # save results to project/name\n",
    "exist_ok=False  # existing project/name ok, do not increment\n",
    "line_thickness=3  # bounding box thickness (pixels)\n",
    "hide_labels=False  # hide labels\n",
    "hide_conf=False  # hide confidences\n",
    "half=False  # use FP16 half-precision inference\n",
    "dnn=False  # use OpenCV DNN for ONNX inference\n",
    "\n",
    "\n",
    "# Load model\n",
    "# device = select_device(device)\n",
    "print(device)\n",
    "model = DetectMultiBackend(weights, device=device, dnn=dnn)\n",
    "stride, names, pt, jit, onnx, engine = model.stride, model.names, model.pt, model.jit, model.onnx, model.engine\n",
    "imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "# Half\n",
    "# half &= (pt or jit or engine) and device.type != 'cpu'  # half precision only supported by PyTorch on CUDA\n",
    "# if pt or jit:\n",
    "    # model.model.half() if half else model.model.float()\n",
    "\n",
    "# Dataloader\n",
    "# if webcam:\n",
    "    # view_img = check_imshow()\n",
    "    # cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "    # dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "    # bs = len(dataset)  # batch_size\n",
    "# else:\n",
    "    # dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt)\n",
    "    # bs = 1  # batch_size\n",
    "# vid_path, vid_writer = [None] * bs, [None] * bs\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    model.warmup(imgsz=(1, 3, *imgsz), half=half)  # warmup\n",
    "    dt, seen = [0.0, 0.0, 0.0], 0\n",
    "# for path, im, im0s, vid_cap, s in dataset:\n",
    "    # t1 = time_sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674c238-5535-4224-bd6f-c14e6e90ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec0244-1e19-4b4b-a83d-5b86cf06bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('input/train.csv')\n",
    "df = df[['image_id','annotations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1042051-1765-41af-8301-556b77b263ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34439736-e59e-421c-9360-fe6ed9c4ea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "impath = glob.glob(\"convertor/fold0/images/val2017/*.jpg\")\n",
    "# path = impath[103]\n",
    "for path in impath[:20]:\n",
    "    im = cv2.imread(path)\n",
    "    display(Image.fromarray(cv2.cvtColor(im,cv2.COLOR_BGR2RGB)))\n",
    "    im = cv2.resize(im, dsize=(1280,736))\n",
    "    img0 = im.copy()\n",
    "\n",
    "    im = im.transpose((2, 0, 1))[::-1] # convert to CHW, RGB\n",
    "\n",
    "    im = torch.from_numpy(im.copy()).to(device)\n",
    "    im = im.half() if half else im.float()  # uint8 to fp16/32\n",
    "    im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "    if len(im.shape) == 3:\n",
    "        im = im[None]  # expand for batch dim\n",
    "    # print(im.shape)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        pred = model(im)\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det) # NMS\n",
    "        # pred = pred[0].cpu().numpy()\n",
    "        for det in pred:\n",
    "            det[:, :4] = scale_coords(im.shape[2:], det[:, :4], img0.shape).round()\n",
    "\n",
    "            for bbox in det:\n",
    "                bbox = bbox.cpu().numpy()\n",
    "                x,y,x2,y2 = bbox[:4].astype(np.uint16)\n",
    "                cv2.rectangle(img0, [x,y], [x2,y2], (255,0,0), 2)\n",
    "            display(Image.fromarray(cv2.cvtColor(img0,cv2.COLOR_BGR2RGB)))\n",
    "    \n",
    "    #answer\n",
    "    image_id = os.path.basename(path).split('.jpg')[0]\n",
    "    video_id, video_frame = image_id.split('-')\n",
    "    img_path = f\"input/train_images/video_{video_id}/{video_frame}.jpg\"\n",
    "    \n",
    "    ans_img = cv2.imread(img_path)\n",
    "    \n",
    "    annos = df[df['image_id']==image_id]['annotations']\n",
    "    annos = ast.literal_eval(annos.item())\n",
    "    for anno in annos:\n",
    "        x = anno['x']\n",
    "        y = anno['y']\n",
    "        w = anno['width']\n",
    "        h = anno['height']\n",
    "        x2 = x+w\n",
    "        y2 = y+h\n",
    "        cv2.rectangle(ans_img, [x,y], [x2,y2], (0,0,255), 2)\n",
    "    display(Image.fromarray(cv2.cvtColor(ans_img,cv2.COLOR_BGR2RGB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa7d87-1f25-4725-bb1d-d2560fad6d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf023973-4194-4e73-9dda-16d2cd5e5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb16488-f463-4bf1-8c28-e56335f7d621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7.10",
   "language": "python",
   "name": "python3.7.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
